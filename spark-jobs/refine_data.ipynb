{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2198fe0e",
   "metadata": {},
   "source": [
    "### Refining the data before curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff354d9",
   "metadata": {},
   "source": [
    "#### Costumers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1a31c",
   "metadata": {},
   "source": [
    "O que √© Particionamento no Spark?\n",
    "√â uma t√©cnica de organiza√ß√£o de dados que:\n",
    "\n",
    "Divide fisicamente os arquivos por valores de colunas\n",
    "\n",
    "Cria subpastas autom√°ticas no formato nome_coluna=valor\n",
    "\n",
    "Melhora performance em consultas filtradas por essas colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6064d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_config.py\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def get_spark_session(app_name):\n",
    "    \"\"\"Configura√ß√£o compartilhada por todos os scripts\"\"\"\n",
    "    # 1. Configura√ß√£o do Hadoop para Windows\n",
    "    hadoop_path = \"C:\\\\hadoop\"\n",
    "    os.environ['HADOOP_HOME'] = hadoop_path\n",
    "    os.environ['PATH'] = f\"{os.environ['PATH']};{hadoop_path}\\\\bin\"\n",
    "    os.environ['HADOOP_USER_NAME'] = \"user\"\n",
    "    \n",
    "    # 2. Configura√ß√µes essenciais do Spark\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "246d60e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Lendo dados de: c:\\Users\\pacie\\Desktop\\dataplatform-pipelines\\data_lake\\parquet\\brazilian_ecommerce_v4\\olist_customers_dataset\n",
      "‚ùå Erro: An error occurred while calling o62.parquet.\n",
      ": java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\n",
      "\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:178)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:134)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:563)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:420)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# customer_refinement_fixed.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "# 1. Configura√ß√£o de caminhos ABSOLUTOS\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(os.getcwd()))  # Volta um n√≠vel para o projeto\n",
    "INPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"parquet\", \"brazilian_ecommerce_v4\", \"olist_customers_dataset\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"refined\", \"customers\")  # Pasta definitiva\n",
    "\n",
    "# 2. Fun√ß√£o de transforma√ß√£o\n",
    "def transform_customers(df):\n",
    "    # Limpeza\n",
    "    df_clean = (df\n",
    "        .withColumn(\"customer_city_clean\", lower(trim(col(\"customer_city\"))))\n",
    "        .withColumn(\"customer_state_clean\", upper(trim(col(\"customer_state\"))))\n",
    "        .dropDuplicates([\"customer_id\"])\n",
    "    )\n",
    "    \n",
    "    # Cria regi√£o (exemplo simplificado)\n",
    "    return df_clean.withColumn(\n",
    "        \"customer_region\",\n",
    "        when(col(\"customer_state_clean\").isin([\"SP\", \"RJ\", \"MG\", \"ES\"]), \"Sudeste\")\n",
    "        .when(col(\"customer_state_clean\").isin([\"RS\", \"SC\", \"PR\"]), \"Sul\")\n",
    "        .when(col(\"customer_state_clean\").isin([\"MT\", \"MS\", \"GO\", \"DF\"]), \"Centro-Oeste\")\n",
    "        .when(col(\"customer_state_clean\").isin([\"AM\", \"PA\", \"AC\", \"RO\", \"RR\", \"AP\", \"TO\"]), \"Norte\")\n",
    "        .when(col(\"customer_state_clean\").isin([\"BA\", \"SE\", \"AL\", \"PE\", \"PB\", \"RN\", \"CE\", \"PI\", \"MA\"]), \"Nordeste\")\n",
    "        .otherwise(\"Outros\")  # Apenas para casos extremos n√£o mapeados\n",
    "    )\n",
    "\n",
    "# 3. Execu√ß√£o principal\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CustomerRefinement\") \\\n",
    "        .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        # Garante que a pasta de output existe\n",
    "        os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "        \n",
    "        print(\"üìÇ Lendo dados de:\", INPUT_PATH)\n",
    "        customers_df = spark.read.parquet(INPUT_PATH)\n",
    "        \n",
    "        print(\"üîÑ Transformando dados...\")\n",
    "        customers_refined = transform_customers(customers_df)\n",
    "        \n",
    "        print(\"üíæ Salvando em:\", OUTPUT_PATH)\n",
    "        (customers_refined.write\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"customer_region\", \"customer_state_clean\")  # Parti√ß√£o dupla\n",
    "            .parquet(OUTPUT_PATH))\n",
    "        \n",
    "        # Verifica√ß√£o\n",
    "        print(\"‚úÖ Estrutura criada:\")\n",
    "        for root, dirs, files in os.walk(OUTPUT_PATH):\n",
    "            print(f\"üìÅ {root.replace(BASE_DIR, '...')}\")\n",
    "            for dir in dirs:\n",
    "                if \"=\" in dir:  # Mostra apenas pastas de parti√ß√£o\n",
    "                    print(f\"   ‚îî‚îÄ‚îÄ {dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48202e16",
   "metadata": {},
   "source": [
    "### Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0187ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Lendo dados de geolocaliza√ß√£o...\n",
      "üõ†Ô∏è Processando dados...\n",
      "üíæ Salvando em: c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\refined\\geolocation\n",
      "‚úÖ Dados salvos! Estrutura:\n",
      "Total de registros: 1000163\n",
      "\n",
      "Amostra dos dados transformados:\n",
      "+---------------------------+------------------+-------------------+----------------+-----------------+----------------------+-----------------------+--------+--------------------+\n",
      "|geolocation_zip_code_prefix|   geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|geolocation_city_clean|geolocation_state_clean|  region|           geo_point|\n",
      "+---------------------------+------------------+-------------------+----------------+-----------------+----------------------+-----------------------+--------+--------------------+\n",
      "|                      50760|-8.072588074383969| -34.91359211518264|          recife|               PE|                recife|                     PE|Nordeste|POINT(-34.9135921...|\n",
      "|                      50740|-8.042627210658422|-34.946004300305844|          recife|               PE|                recife|                     PE|Nordeste|POINT(-34.9460043...|\n",
      "|                      50751|-8.066622635581963|-34.917061201223355|          recife|               PE|                recife|                     PE|Nordeste|POINT(-34.9170612...|\n",
      "+---------------------------+------------------+-------------------+----------------+-----------------+----------------------+-----------------------+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Exemplo de estrutura gerada:\n",
      "üìÅ c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\refined\\geolocation\\region=Sudeste\n",
      "   Estados: ['geolocation_state_clean=ES', 'geolocation_state_clean=MG', 'geolocation_state_clean=RJ', 'geolocation_state_clean=SP']\n",
      "üìÅ c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\refined\\geolocation\\region=Nordeste\n",
      "   Estados: ['geolocation_state_clean=AL', 'geolocation_state_clean=BA', 'geolocation_state_clean=CE', 'geolocation_state_clean=MA', 'geolocation_state_clean=PB', 'geolocation_state_clean=PE', 'geolocation_state_clean=PI', 'geolocation_state_clean=RN', 'geolocation_state_clean=SE']\n",
      "üìÅ c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\refined\\geolocation\\region=Sul\n",
      "   Estados: ['geolocation_state_clean=PR', 'geolocation_state_clean=RS', 'geolocation_state_clean=SC']\n"
     ]
    }
   ],
   "source": [
    "# geolocation_refinement.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "# Configura√ß√£o de caminhos\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(os.getcwd()))  # Volta um n√≠vel\n",
    "INPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"parquet\", \"brazilian_ecommerce_v4\", \"olist_geolocation_dataset\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"refined\", \"geolocation\")\n",
    "\n",
    "def transform_geolocation_data(df):\n",
    "    \"\"\"Transforma os dados de geolocaliza√ß√£o com:\n",
    "    - Limpeza de textos\n",
    "    - Categoriza√ß√£o por regi√£o\n",
    "    - Controle de qualidade\n",
    "    \"\"\"\n",
    "    # Limpeza e padroniza√ß√£o\n",
    "    df_clean = (df\n",
    "        .withColumn(\"geolocation_city_clean\", lower(trim(col(\"geolocation_city\"))))\n",
    "        .withColumn(\"geolocation_state_clean\", upper(trim(col(\"geolocation_state\"))))\n",
    "        .na.drop(subset=[\"geolocation_zip_code_prefix\"])  # Remove registros sem CEP\n",
    "    )\n",
    "    \n",
    "    # Cria regi√£o geogr√°fica (mesma l√≥gica dos clientes)\n",
    "    df_with_region = df_clean.withColumn(\n",
    "        \"region\",\n",
    "        when(col(\"geolocation_state_clean\").isin([\"SP\", \"RJ\", \"MG\", \"ES\"]), \"Sudeste\")\n",
    "        .when(col(\"geolocation_state_clean\").isin([\"RS\", \"SC\", \"PR\"]), \"Sul\")\n",
    "        .when(col(\"geolocation_state_clean\").isin([\"MT\", \"MS\", \"GO\", \"DF\"]), \"Centro-Oeste\")\n",
    "        .when(col(\"geolocation_state_clean\").isin([\"AM\", \"PA\", \"AC\", \"RO\", \"RR\", \"AP\", \"TO\"]), \"Norte\")\n",
    "        .otherwise(\"Nordeste\")\n",
    "    )\n",
    "    \n",
    "    # Cria ponto geogr√°fico (lat+lng)\n",
    "    return df_with_region.withColumn(\n",
    "        \"geo_point\",\n",
    "        format_string(\"POINT(%s %s)\", col(\"geolocation_lng\"), col(\"geolocation_lat\"))\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"GeolocationRefinement\").getOrCreate()\n",
    "    \n",
    "    try:\n",
    "        # Leitura\n",
    "        print(\"üìç Lendo dados de geolocaliza√ß√£o...\")\n",
    "        geo_df = spark.read.parquet(INPUT_PATH)\n",
    "        \n",
    "        # Transforma√ß√£o\n",
    "        print(\"üõ†Ô∏è Processando dados...\")\n",
    "        geo_refined = transform_geolocation_data(geo_df)\n",
    "        \n",
    "        # Escrita com particionamento por regi√£o/estado\n",
    "        print(\"üíæ Salvando em:\", OUTPUT_PATH)\n",
    "        (geo_refined.write\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"region\", \"geolocation_state_clean\")\n",
    "            .option(\"compression\", \"snappy\")\n",
    "            .parquet(OUTPUT_PATH))\n",
    "        \n",
    "        # Verifica√ß√£o\n",
    "        print(\"‚úÖ Dados salvos! Estrutura:\")\n",
    "        print(f\"Total de registros: {geo_refined.count()}\")\n",
    "        print(\"\\nAmostra dos dados transformados:\")\n",
    "        geo_refined.show(3)\n",
    "        \n",
    "        print(\"\\nExemplo de estrutura gerada:\")\n",
    "        for region in [\"Sudeste\", \"Nordeste\", \"Sul\"]:\n",
    "            path = os.path.join(OUTPUT_PATH, f\"region={region}\")\n",
    "            if os.path.exists(path):\n",
    "                print(f\"üìÅ {path}\")\n",
    "                print(f\"   Estados: {os.listdir(path)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Falha no processamento: {str(e)}\")\n",
    "        if 'geo_df' in locals():\n",
    "            print(\"\\nEsquema original:\", geo_df.printSchema())\n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7763f",
   "metadata": {},
   "source": [
    "### Order Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef51d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Verificando HADOOP_HOME em: C:\\hadoop\n",
      "‚úîÔ∏è Verificando winutils.exe em: C:\\hadoop\\bin\\winutils.exe\n",
      "üì¶ Lendo dados de order_items...\n",
      "üõ†Ô∏è Enriquecendo dados...\n",
      "üíæ Salvando dados refinados...\n",
      "‚úÖ Processo conclu√≠do! Amostra dos dados:\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+-----------+-----------+-------------+----------------+--------------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|total_value|price_range|freight_ratio|freight_category|shipping_time_period|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+-----------+-----------+-------------+----------------+--------------------+\n",
      "|8ac26cb701a7887cc...|            1|4ebb87ba41ca44632...|7a67c85e85bb2ce85...|2017-05-22 16:05:14|109.99|        18.02|     128.01|    100-150|         0.16|           M√©dio|               Tarde|\n",
      "|8ac2728285fd4228f...|            1|8b90be4893a4277a9...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|109.99|         8.27|     118.26|    100-150|         0.08|           Baixo|               Tarde|\n",
      "|8ac2728285fd4228f...|            2|fa94f25a73969e3a2...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|109.99|        16.55|     126.54|    100-150|         0.15|           M√©dio|               Tarde|\n",
      "|8ac2728285fd4228f...|            3|b01cedfa96d891427...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|259.99|        21.01|      281.0|       150+|         0.08|           Baixo|               Tarde|\n",
      "|8ac2728285fd4228f...|            4|fa94f25a73969e3a2...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|109.99|        16.55|     126.54|    100-150|         0.15|           M√©dio|               Tarde|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+-----------+-----------+-------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "üìä Estat√≠sticas:\n",
      "‚ùå Erro: name 'countDistinct' is not defined\n"
     ]
    }
   ],
   "source": [
    "# order_items_refinement.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "def get_spark_session(app_name):\n",
    "    \"\"\"\n",
    "    Configura√ß√£o compartilhada por todos os scripts, com op√ß√µes para Hadoop.\n",
    "    \n",
    "    Verifica a presen√ßa de 'winutils.exe' para evitar 'UnsatisfiedLinkError' no Windows.\n",
    "    \"\"\"\n",
    "    # 1. Configura√ß√£o do Hadoop para Windows (ajuste se necess√°rio para outros SOs)\n",
    "    # ATEN√á√ÉO: Verifique o caminho para o seu diret√≥rio Hadoop e baixe o bin√°rio\n",
    "    # do winutils.exe compat√≠vel com a sua vers√£o do Spark/Hadoop.\n",
    "    hadoop_path = \"C:\\\\hadoop\"\n",
    "    winutils_path = os.path.join(hadoop_path, \"bin\", \"winutils.exe\")\n",
    "    \n",
    "    # Adicionando debug para verificar os caminhos\n",
    "    print(f\"‚úîÔ∏è Verificando HADOOP_HOME em: {hadoop_path}\")\n",
    "    print(f\"‚úîÔ∏è Verificando winutils.exe em: {winutils_path}\")\n",
    "    \n",
    "    # Verifica se o winutils.exe existe antes de tentar configurar as vari√°veis de ambiente\n",
    "    if not os.path.exists(winutils_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"‚ùå Erro: O arquivo winutils.exe n√£o foi encontrado em '{winutils_path}'. \"\n",
    "            f\"Isso geralmente indica um problema com a vari√°vel de ambiente HADOOP_HOME \"\n",
    "            f\"ou que o bin√°rio necess√°rio n√£o est√° no local correto. \"\n",
    "            f\"Por favor, verifique se a pasta '{hadoop_path}' cont√©m a estrutura de pastas \"\n",
    "            f\"'bin' e se o 'winutils.exe' est√° dentro dela.\"\n",
    "        )\n",
    "        \n",
    "    os.environ['HADOOP_HOME'] = hadoop_path\n",
    "    os.environ['PATH'] = f\"{os.environ['PATH']};{hadoop_path}\\\\bin\"\n",
    "    os.environ['HADOOP_USER_NAME'] = \"user\"\n",
    "    \n",
    "    # 2. Configura√ß√µes essenciais do Spark\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "# Configura√ß√£o de caminhos\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "INPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"parquet\", \"brazilian_ecommerce_v4\", \"olist_order_items_dataset\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"refined\", \"order_items\")\n",
    "\n",
    "def transform_order_items(df):\n",
    "    \"\"\"Transforma√ß√µes principais para order_items\"\"\"\n",
    "    # 1. Limpeza b√°sica\n",
    "    df_clean = df.withColumn(\n",
    "        \"shipping_limit_date\", \n",
    "        to_timestamp(col(\"shipping_limit_date\"))\n",
    "    ).na.drop(subset=[\"order_id\", \"product_id\", \"seller_id\"])\n",
    "    \n",
    "    # 2. C√°lculo de valores totais\n",
    "    df_with_totals = df_clean.withColumn(\n",
    "        \"total_value\", \n",
    "        round(col(\"price\") + col(\"freight_value\"), 2)\n",
    "    )\n",
    "    \n",
    "    # 3. Categoriza√ß√£o por faixa de pre√ßo\n",
    "    df_with_price_range = df_with_totals.withColumn(\n",
    "        \"price_range\",\n",
    "        when(col(\"price\") < 50, \"0-50\")\n",
    "        .when((col(\"price\") >= 50) & (col(\"price\") < 100), \"50-100\")\n",
    "        .when((col(\"price\") >= 100) & (col(\"price\") < 150), \"100-150\") \n",
    "        .otherwise(\"150+\")\n",
    "    )\n",
    "    \n",
    "    # 4. Categoriza√ß√£o por frete\n",
    "    df_with_freight_category = df_with_price_range.withColumn(\n",
    "        \"freight_ratio\",\n",
    "        round(col(\"freight_value\") / col(\"price\"), 2)\n",
    "    ).withColumn(\n",
    "        \"freight_category\",\n",
    "        when(col(\"freight_ratio\") < 0.1, \"Baixo\")\n",
    "        .when((col(\"freight_ratio\") >= 0.1) & (col(\"freight_ratio\") < 0.25), \"M√©dio\")\n",
    "        .otherwise(\"Alto\")\n",
    "    )\n",
    "    \n",
    "    # 5. Adi√ß√£o de per√≠odo do dia (baseado no hor√°rio do shipping)\n",
    "    return df_with_freight_category.withColumn(\n",
    "        \"shipping_time_period\",\n",
    "        when(hour(col(\"shipping_limit_date\")).between(6, 11), \"Manh√£\")\n",
    "        .when(hour(col(\"shipping_limit_date\")).between(12, 17), \"Tarde\")\n",
    "        .when(hour(col(\"shipping_limit_date\")).between(18, 23), \"Noite\")\n",
    "        .otherwise(\"Madrugada\")\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Inicia a sess√£o Spark usando a fun√ß√£o de configura√ß√£o\n",
    "    spark = get_spark_session(\"OrderItemsRefinement\")\n",
    "    \n",
    "    try:\n",
    "        # Leitura\n",
    "        print(\"üì¶ Lendo dados de order_items...\")\n",
    "        order_items_df = spark.read.parquet(INPUT_PATH)\n",
    "        \n",
    "        # Transforma√ß√£o\n",
    "        print(\"üõ†Ô∏è Enriquecendo dados...\")\n",
    "        order_items_refined = transform_order_items(order_items_df)\n",
    "        \n",
    "        # Escrita com particionamento\n",
    "        print(\"üíæ Salvando dados refinados...\")\n",
    "        (order_items_refined.write\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"price_range\", \"freight_category\")  # Particionamento estrat√©gico\n",
    "            .parquet(OUTPUT_PATH))\n",
    "        \n",
    "        # Verifica√ß√£o\n",
    "        print(\"‚úÖ Processo conclu√≠do! Amostra dos dados:\")\n",
    "        order_items_refined.show(5)\n",
    "        \n",
    "        print(\"\\nüìä Estat√≠sticas:\")\n",
    "        order_items_refined.select(\n",
    "            mean(\"price\").alias(\"avg_price\"),\n",
    "            mean(\"freight_value\").alias(\"avg_freight\"),\n",
    "            countDistinct(\"order_id\").alias(\"unique_orders\")\n",
    "        ).show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b47e004",
   "metadata": {},
   "source": [
    "### Order payments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73f5fa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Verificando HADOOP_HOME em: C:\\hadoop\n",
      "‚úîÔ∏è Verificando winutils.exe em: C:\\hadoop\\bin\\winutils.exe\n",
      "üì¶ Lendo dados de order_payments...\n",
      "üõ†Ô∏è Enriquecendo dados...\n",
      "üíæ Salvando dados refinados...\n",
      "‚úÖ Processo conclu√≠do! Amostra dos dados:\n",
      "+--------------------+------------------+------------+--------------------+-------------+---------------------+-------------------+--------------------+---------------------+\n",
      "|            order_id|payment_sequential|payment_type|payment_installments|payment_value|payment_type_category|payment_value_range|installment_category|is_high_value_payment|\n",
      "+--------------------+------------------+------------+--------------------+-------------+---------------------+-------------------+--------------------+---------------------+\n",
      "|b81ef226f3fe1789b...|                 1| credit_card|                   8|        99.33|               cart√£o|             50-100|     parcelado_m√©dio|                false|\n",
      "|a9810da82917af2d9...|                 1| credit_card|                   1|        24.39|               cart√£o|               0-50|             √† vista|                false|\n",
      "|25e8ea4e93396b6fa...|                 1| credit_card|                   1|        65.71|               cart√£o|             50-100|             √† vista|                false|\n",
      "|ba78997921bbcdc13...|                 1| credit_card|                   8|       107.78|               cart√£o|            100-200|     parcelado_m√©dio|                false|\n",
      "|42fdf880ba16b47b5...|                 1| credit_card|                   2|       128.45|               cart√£o|            100-200|     parcelado_curto|                false|\n",
      "+--------------------+------------------+------------+--------------------+-------------+---------------------+-------------------+--------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "üìä Estat√≠sticas:\n",
      "‚ùå Erro: name 'countDistinct' is not defined\n"
     ]
    }
   ],
   "source": [
    "# order_payments_refinement.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "def get_spark_session(app_name):\n",
    "    \"\"\"\n",
    "    Configura√ß√£o compartilhada por todos os scripts, com op√ß√µes para Hadoop.\n",
    "    \n",
    "    Verifica a presen√ßa de 'winutils.exe' para evitar 'UnsatisfiedLinkError' no Windows.\n",
    "    \"\"\"\n",
    "    # 1. Configura√ß√£o do Hadoop para Windows (ajuste se necess√°rio para outros SOs)\n",
    "    hadoop_path = \"C:\\\\hadoop\"\n",
    "    winutils_path = os.path.join(hadoop_path, \"bin\", \"winutils.exe\")\n",
    "    \n",
    "    # Adicionando debug para verificar os caminhos\n",
    "    print(f\"‚úîÔ∏è Verificando HADOOP_HOME em: {hadoop_path}\")\n",
    "    print(f\"‚úîÔ∏è Verificando winutils.exe em: {winutils_path}\")\n",
    "    \n",
    "    # Verifica se o winutils.exe existe antes de tentar configurar as vari√°veis de ambiente\n",
    "    if not os.path.exists(winutils_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"‚ùå Erro: O arquivo winutils.exe n√£o foi encontrado em '{winutils_path}'. \"\n",
    "            f\"Por favor, verifique se a pasta '{hadoop_path}' cont√©m a estrutura de pastas \"\n",
    "            f\"'bin' e se o 'winutils.exe' est√° dentro dela.\"\n",
    "        )\n",
    "        \n",
    "    os.environ['HADOOP_HOME'] = hadoop_path\n",
    "    os.environ['PATH'] = f\"{os.environ['PATH']};{hadoop_path}\\\\bin\"\n",
    "    os.environ['HADOOP_USER_NAME'] = \"user\"\n",
    "    \n",
    "    # 2. Configura√ß√µes essenciais do Spark\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Configura√ß√£o de caminhos\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "INPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"parquet\", \"brazilian_ecommerce_v4\", \"olist_order_payments_dataset\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"refined\", \"order_payments\")\n",
    "\n",
    "def transform_order_payments(df):\n",
    "    \"\"\"Transforma√ß√µes principais para order_payments\"\"\"\n",
    "    # 1. Limpeza b√°sica\n",
    "    df_clean = df.na.drop(subset=[\"order_id\", \"payment_type\", \"payment_value\"])\n",
    "    \n",
    "    # 2. Categoriza√ß√£o de tipos de pagamento\n",
    "    df_with_payment_categories = df_clean.withColumn(\n",
    "        \"payment_type_category\",\n",
    "        when(col(\"payment_type\").isin([\"credit_card\", \"debit_card\"]), \"cart√£o\")\n",
    "        .when(col(\"payment_type\") == \"boleto\", \"boleto\")\n",
    "        .when(col(\"payment_type\") == \"voucher\", \"voucher\")\n",
    "        .otherwise(\"outros\")\n",
    "    )\n",
    "    \n",
    "    # 3. Categoriza√ß√£o por valor de pagamento\n",
    "    df_with_value_ranges = df_with_payment_categories.withColumn(\n",
    "        \"payment_value_range\",\n",
    "        when(col(\"payment_value\") < 50, \"0-50\")\n",
    "        .when((col(\"payment_value\") >= 50) & (col(\"payment_value\") < 100), \"50-100\")\n",
    "        .when((col(\"payment_value\") >= 100) & (col(\"payment_value\") < 200), \"100-200\")\n",
    "        .otherwise(\"200+\")\n",
    "    )\n",
    "    \n",
    "    # 4. Categoriza√ß√£o por parcelamento\n",
    "    df_with_installment_categories = df_with_value_ranges.withColumn(\n",
    "        \"installment_category\",\n",
    "        when(col(\"payment_installments\") == 1, \"√† vista\")\n",
    "        .when((col(\"payment_installments\") > 1) & (col(\"payment_installments\") <= 6), \"parcelado_curto\")\n",
    "        .when((col(\"payment_installments\") > 6) & (col(\"payment_installments\") <= 12), \"parcelado_m√©dio\")\n",
    "        .otherwise(\"parcelado_longo\")\n",
    "    )\n",
    "    \n",
    "    # 5. Flag para pagamentos com valores discrepantes\n",
    "    return df_with_installment_categories.withColumn(\n",
    "        \"is_high_value_payment\",\n",
    "        when(col(\"payment_value\") > 500, True).otherwise(False)\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Inicia a sess√£o Spark usando a fun√ß√£o de configura√ß√£o\n",
    "    spark = get_spark_session(\"OrderPaymentsRefinement\")\n",
    "    \n",
    "    try:\n",
    "        # Leitura\n",
    "        print(\"üì¶ Lendo dados de order_payments...\")\n",
    "        order_payments_df = spark.read.parquet(INPUT_PATH)\n",
    "        \n",
    "        # Transforma√ß√£o\n",
    "        print(\"üõ†Ô∏è Enriquecendo dados...\")\n",
    "        order_payments_refined = transform_order_payments(order_payments_df)\n",
    "        \n",
    "        # Escrita com particionamento\n",
    "        print(\"üíæ Salvando dados refinados...\")\n",
    "        (order_payments_refined.write\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"payment_type_category\", \"payment_value_range\")  # Particionamento estrat√©gico\n",
    "            .parquet(OUTPUT_PATH))\n",
    "        \n",
    "        # Verifica√ß√£o\n",
    "        print(\"‚úÖ Processo conclu√≠do! Amostra dos dados:\")\n",
    "        order_payments_refined.show(5)\n",
    "        \n",
    "        print(\"\\nüìä Estat√≠sticas:\")\n",
    "        order_payments_refined.select(\n",
    "            count(\"*\").alias(\"total_payments\"),\n",
    "            mean(\"payment_value\").alias(\"avg_payment\"),\n",
    "            countDistinct(\"order_id\").alias(\"unique_orders\"),\n",
    "            sum(when(col(\"is_high_value_payment\") == True, 1).otherwise(0)).alias(\"high_value_payments\")\n",
    "        ).show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6965ddbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Lendo dados de: c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\parquet\\brazilian_ecommerce_v4\\olist_order_reviews_dataset\n",
      "üíæ Salvando dados refinados...\n"
     ]
    },
    {
     "ename": "DateTimeException",
     "evalue": "[CANNOT_PARSE_TIMESTAMP] Unparseable date: \" produto ainda n√£o chegou ao destino.\". Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDateTimeException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 82\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# SALVAR\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müíæ Salvando dados refinados...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreview_score_category\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Processo conclu√≠do! Dados salvos em \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\Users\\pacie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:2003\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   2001\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 2003\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pacie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\pacie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mDateTimeException\u001b[0m: [CANNOT_PARSE_TIMESTAMP] Unparseable date: \" produto ainda n√£o chegou ao destino.\". Use `try_to_timestamp` to tolerate invalid input string and return NULL instead. SQLSTATE: 22007"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURA√á√ïES\n",
    "# ==========================================\n",
    "INPUT_PATH = r\"c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\parquet\\brazilian_ecommerce_v4\\olist_order_reviews_dataset\"\n",
    "OUTPUT_PATH = r\"c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\refined\\order_reviews\"\n",
    "HADOOP_HOME = r\"C:\\hadoop\"\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = HADOOP_HOME\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(HADOOP_HOME, \"bin\")\n",
    "\n",
    "# ==========================================\n",
    "# INICIALIZA√á√ÉO DO SPARK\n",
    "# ==========================================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OrderReviewsRefinement\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"üì¶ Lendo dados de: {INPUT_PATH}\")\n",
    "df = spark.read.parquet(INPUT_PATH)\n",
    "\n",
    "# ==========================================\n",
    "# LIMPEZA\n",
    "# ==========================================\n",
    "df = df.withColumn(\"review_comment_title\", F.trim(F.col(\"review_comment_title\"))) \\\n",
    "       .withColumn(\"review_comment_message\", F.trim(F.col(\"review_comment_message\")))\n",
    "\n",
    "# Datas como timestamp (tolerante a erro)\n",
    "df = df.withColumn(\"review_creation_ts\", F.to_timestamp(\"review_creation_date\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "       .withColumn(\"review_answer_ts\", F.to_timestamp(\"review_answer_timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# ==========================================\n",
    "# SCORE NUM√âRICO COM TRY_CAST\n",
    "# ==========================================\n",
    "df = df.withColumn(\"review_score_int\", F.expr(\"try_cast(review_score as int)\"))\n",
    "\n",
    "# ==========================================\n",
    "# ENRIQUECIMENTO\n",
    "# ==========================================\n",
    "df = df.withColumn(\n",
    "    \"review_score_category\",\n",
    "    F.when(F.col(\"review_score_int\") >= 4, \"positive\")\n",
    "     .when(F.col(\"review_score_int\") == 3, \"neutral\")\n",
    "     .when(F.col(\"review_score_int\") <= 2, \"negative\")\n",
    "     .otherwise(None)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"sentiment\",\n",
    "    F.when(F.col(\"review_score_int\") >= 4, \"positivo\")\n",
    "     .when(F.col(\"review_score_int\") == 3, \"neutro\")\n",
    "     .when(F.col(\"review_score_int\") <= 2, \"negativo\")\n",
    "     .otherwise(None)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"has_comment\",\n",
    "    F.when(F.col(\"review_comment_message\").isNotNull() & (F.col(\"review_comment_message\") != \"\"), True)\n",
    "     .otherwise(False)\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"response_time_hours\",\n",
    "    (F.unix_timestamp(\"review_answer_ts\") - F.unix_timestamp(\"review_creation_ts\")) / 3600\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"response_speed_category\",\n",
    "    F.when(F.col(\"response_time_hours\") <= 24, \"fast\")\n",
    "     .when((F.col(\"response_time_hours\") > 24) & (F.col(\"response_time_hours\") <= 72), \"medium\")\n",
    "     .when(F.col(\"response_time_hours\") > 72, \"slow\")\n",
    "     .otherwise(None)\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# SALVAR\n",
    "# ==========================================\n",
    "print(\"üíæ Salvando dados refinados...\")\n",
    "df.write.mode(\"overwrite\").partitionBy(\"review_score_category\", \"sentiment\").parquet(OUTPUT_PATH)\n",
    "\n",
    "print(f\"‚úÖ Processo conclu√≠do! Dados salvos em {OUTPUT_PATH}\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639e086",
   "metadata": {},
   "source": [
    "### Orders Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders_refinement.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "def get_spark_session(app_name):\n",
    "    \"\"\"\n",
    "    Configura√ß√£o compartilhada por todos os scripts, com op√ß√µes para Hadoop.\n",
    "    \"\"\"\n",
    "    hadoop_path = \"C:\\\\hadoop\"\n",
    "    winutils_path = os.path.join(hadoop_path, \"bin\", \"winutils.exe\")\n",
    "    \n",
    "    print(f\"‚úîÔ∏è Verificando HADOOP_HOME em: {hadoop_path}\")\n",
    "    print(f\"‚úîÔ∏è Verificando winutils.exe em: {winutils_path}\")\n",
    "    \n",
    "    if not os.path.exists(winutils_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"‚ùå Erro: winutils.exe n√£o encontrado em '{winutils_path}'. \"\n",
    "            f\"Verifique se o Hadoop est√° corretamente instalado.\"\n",
    "        )\n",
    "        \n",
    "    os.environ['HADOOP_HOME'] = hadoop_path\n",
    "    os.environ['PATH'] = f\"{os.environ['PATH']};{hadoop_path}\\\\bin\"\n",
    "    os.environ['HADOOP_USER_NAME'] = \"user\"\n",
    "    \n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Configura√ß√£o de caminhos\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "INPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"parquet\", \"brazilian_ecommerce_v4\", \"olist_orders_dataset\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"refined\", \"orders\")\n",
    "\n",
    "def transform_orders(df):\n",
    "    \"\"\"Transforma√ß√µes principais para orders\"\"\"\n",
    "    # 1. Limpeza b√°sica e convers√£o de datas\n",
    "    df_clean = df.withColumn(\n",
    "        \"order_purchase_timestamp\", \n",
    "        to_timestamp(col(\"order_purchase_timestamp\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    "    ).withColumn(\n",
    "        \"order_approved_at\",\n",
    "        to_timestamp(col(\"order_approved_at\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    "    ).withColumn(\n",
    "        \"order_delivered_carrier_date\",\n",
    "        to_timestamp(col(\"order_delivered_carrier_date\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    "    ).withColumn(\n",
    "        \"order_delivered_customer_date\",\n",
    "        to_timestamp(col(\"order_delivered_customer_date\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    "    ).withColumn(\n",
    "        \"order_estimated_delivery_date\",\n",
    "        to_timestamp(col(\"order_estimated_delivery_date\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    "    ).na.drop(subset=[\"order_id\", \"customer_id\", \"order_status\"])\n",
    "    \n",
    "    # 2. C√°lculo de tempos de processamento\n",
    "    df_with_times = df_clean.withColumn(\n",
    "        \"approval_time_hours\",\n",
    "        round((col(\"order_approved_at\").cast(\"long\") - col(\"order_purchase_timestamp\").cast(\"long\")) / 3600,\n",
    "        2\n",
    "    )).withColumn(\n",
    "        \"processing_time_hours\",\n",
    "        round((col(\"order_delivered_carrier_date\").cast(\"long\") - col(\"order_approved_at\").cast(\"long\")) / 3600,\n",
    "        2\n",
    "    )).withColumn(\n",
    "        \"shipping_time_hours\",\n",
    "        round((col(\"order_delivered_customer_date\").cast(\"long\") - col(\"order_delivered_carrier_date\").cast(\"long\")) / 3600,\n",
    "        2\n",
    "    )).withColumn(\n",
    "        \"total_delivery_time_hours\",\n",
    "        round((col(\"order_delivered_customer_date\").cast(\"long\") - col(\"order_purchase_timestamp\").cast(\"long\")) / 3600,\n",
    "        2\n",
    "    )).withColumn(\n",
    "        \"delivery_delay_hours\",\n",
    "        round((col(\"order_delivered_customer_date\").cast(\"long\") - col(\"order_estimated_delivery_date\").cast(\"long\")) / 3600,\n",
    "        2\n",
    "    ))\n",
    "    \n",
    "    # 3. Categoriza√ß√£o de status\n",
    "    df_with_status = df_with_times.withColumn(\n",
    "        \"status_category\",\n",
    "        when(col(\"order_status\") == \"delivered\", \"Entregue\")\n",
    "        .when(col(\"order_status\") == \"shipped\", \"Enviado\")\n",
    "        .when(col(\"order_status\") == \"canceled\", \"Cancelado\")\n",
    "        .when(col(\"order_status\") == \"unavailable\", \"Indispon√≠vel\")\n",
    "        .otherwise(\"Outro\")\n",
    "    )\n",
    "    \n",
    "    # 4. Categoriza√ß√£o de desempenho de entrega\n",
    "    df_with_delivery = df_with_status.withColumn(\n",
    "        \"delivery_performance\",\n",
    "        when(col(\"delivery_delay_hours\").isNull(), \"N√£o entregue\")\n",
    "        .when(col(\"delivery_delay_hours\") <= 0, \"No prazo ou antecipado\")\n",
    "        .when(col(\"delivery_delay_hours\") <= 24, \"Pequeno atraso (<24h)\")\n",
    "        .when(col(\"delivery_delay_hours\") <= 72, \"Atraso moderado (24-72h)\")\n",
    "        .otherwise(\"Grande atraso (>72h)\")\n",
    "    )\n",
    "    \n",
    "    # 5. Categoriza√ß√£o por per√≠odo do dia (compra)\n",
    "    return df_with_delivery.withColumn(\n",
    "        \"purchase_time_period\",\n",
    "        when(hour(col(\"order_purchase_timestamp\")).between(6, 11), \"Manh√£\")\n",
    "        .when(hour(col(\"order_purchase_timestamp\")).between(12, 17), \"Tarde\")\n",
    "        .when(hour(col(\"order_purchase_timestamp\")).between(18, 23), \"Noite\")\n",
    "        .otherwise(\"Madrugada\")\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark_session(\"OrdersRefinement\")\n",
    "    \n",
    "    try:\n",
    "        # Leitura\n",
    "        print(\"üì¶ Lendo dados de orders...\")\n",
    "        orders_df = spark.read.parquet(INPUT_PATH)\n",
    "        \n",
    "        # Transforma√ß√£o\n",
    "        print(\"üõ†Ô∏è Enriquecendo dados...\")\n",
    "        orders_refined = transform_orders(orders_df)\n",
    "        \n",
    "        # Escrita com particionamento\n",
    "        print(\"üíæ Salvando dados refinados...\")\n",
    "        (orders_refined.write\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"status_category\", \"delivery_performance\")  # Particionamento estrat√©gico\n",
    "            .parquet(OUTPUT_PATH))\n",
    "        \n",
    "        # Verifica√ß√£o\n",
    "        print(\"‚úÖ Processo conclu√≠do! Amostra dos dados:\")\n",
    "        orders_refined.show(5)\n",
    "        \n",
    "        print(\"\\nüìä Estat√≠sticas:\")\n",
    "        orders_refined.select(\n",
    "            count(\"*\").alias(\"total_orders\"),\n",
    "            countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "            mean(\"total_delivery_time_hours\").alias(\"avg_delivery_time_hours\"),\n",
    "            mean(\"delivery_delay_hours\").alias(\"avg_delay_hours\"),\n",
    "            sum(when(col(\"delivery_performance\") == \"No prazo ou antecipado\", 1).otherwise(0)).alias(\"on_time_deliveries\")\n",
    "        ).show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57794d8",
   "metadata": {},
   "source": [
    "### Products Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d29164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Lendo dados de products (Parquet)...\n",
      "üõ†Ô∏è Enriquecendo dados...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Salvando dados refinados...\n",
      "‚úÖ Processo conclu√≠do! Amostra dos dados:\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+------------------+---------------------+-------------+--------------+--------------------+\n",
      "|          product_id|product_category_name|product_name_lenght|product_description_lenght|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|product_volume_cm3|product_density_g_cm3|size_category|photo_category|description_category|\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+------------------+---------------------+-------------+--------------+--------------------+\n",
      "|1e9e8ef04dbcff454...|           perfumaria|                 40|                       287|                 1|             225|               16|               10|              14|              2240|               0.1004|        M√©dio|      Uma foto|     Descri√ß√£o longa|\n",
      "|3aa071139cb16b67c...|                artes|                 44|                       276|                 1|            1000|               30|               18|              20|             10800|               0.0926|       Grande|      Uma foto|     Descri√ß√£o longa|\n",
      "|96bd76ec8810374ed...|        esporte_lazer|                 46|                       250|                 1|             154|               18|                9|              15|              2430|               0.0634|        M√©dio|      Uma foto|     Descri√ß√£o longa|\n",
      "|cef67bcfe19066a93...|                bebes|                 27|                       261|                 1|             371|               26|                4|              26|              2704|               0.1372|        M√©dio|      Uma foto|     Descri√ß√£o longa|\n",
      "|9dc1a7de274444849...| utilidades_domest...|                 37|                       402|                 4|             625|               20|               17|              13|              4420|               0.1414|       Grande|  Muitas fotos|     Descri√ß√£o longa|\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+------------------+---------------------+-------------+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "üìä Estat√≠sticas:\n",
      "+--------------+-----------------+-----------------+------------------+-------------------+\n",
      "|total_products|unique_categories|     avg_weight_g|    avg_volume_cm3|        avg_density|\n",
      "+--------------+-----------------+-----------------+------------------+-------------------+\n",
      "|         32341|               73|2276.886181627037|16577.448687424632|0.20332748917748913|\n",
      "+--------------+-----------------+-----------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# products_refinement.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "def get_spark_session(app_name):\n",
    "    \"\"\"Configura√ß√£o do Spark com Hadoop\"\"\"\n",
    "    hadoop_path = \"C:\\\\hadoop\"\n",
    "    os.environ['HADOOP_HOME'] = hadoop_path\n",
    "    os.environ['PATH'] = f\"{os.environ['PATH']};{hadoop_path}\\\\bin\"\n",
    "    \n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def transform_products(df):\n",
    "    \"\"\"Transforma√ß√µes principais para products\"\"\"\n",
    "    # 1. Limpeza b√°sica\n",
    "    df_clean = df.na.drop(subset=[\"product_id\", \"product_category_name\"]) \\\n",
    "        .fillna({\n",
    "            'product_name_lenght': 0,\n",
    "            'product_description_lenght': 0,\n",
    "            'product_photos_qty': 1,\n",
    "            'product_weight_g': 0,\n",
    "            'product_length_cm': 0,\n",
    "            'product_height_cm': 0,\n",
    "            'product_width_cm': 0\n",
    "        })\n",
    "    \n",
    "    # 2. C√°lculo de volume e densidade\n",
    "    df_enriched = df_clean.withColumn(\n",
    "        \"product_volume_cm3\",\n",
    "        round(col(\"product_length_cm\") * col(\"product_height_cm\") * col(\"product_width_cm\"), 2)\n",
    "    ).withColumn(\n",
    "        \"product_density_g_cm3\",\n",
    "        when(col(\"product_volume_cm3\") > 0, \n",
    "             round(col(\"product_weight_g\") / col(\"product_volume_cm3\"), 4))\n",
    "        .otherwise(None)\n",
    "    )\n",
    "    \n",
    "    # 3. Categoriza√ß√µes\n",
    "    return df_enriched.withColumn(\n",
    "        \"size_category\",\n",
    "        when(col(\"product_weight_g\") < 100, \"Pequeno\")\n",
    "        .when((col(\"product_weight_g\") >= 100) & (col(\"product_weight_g\") < 500), \"M√©dio\")\n",
    "        .when((col(\"product_weight_g\") >= 500) & (col(\"product_weight_g\") < 2000), \"Grande\")\n",
    "        .otherwise(\"Muito Grande\")\n",
    "    ).withColumn(\n",
    "        \"photo_category\",\n",
    "        when(col(\"product_photos_qty\") == 0, \"Sem foto\")\n",
    "        .when(col(\"product_photos_qty\") == 1, \"Uma foto\")\n",
    "        .when(col(\"product_photos_qty\") <= 3, \"Poucas fotos\")\n",
    "        .otherwise(\"Muitas fotos\")\n",
    "    ).withColumn(\n",
    "        \"description_category\",\n",
    "        when(col(\"product_description_lenght\") == 0, \"Sem descri√ß√£o\")\n",
    "        .when(col(\"product_description_lenght\") < 50, \"Descri√ß√£o curta\")\n",
    "        .when(col(\"product_description_lenght\") < 200, \"Descri√ß√£o m√©dia\")\n",
    "        .otherwise(\"Descri√ß√£o longa\")\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark_session(\"ProductsRefinement\")\n",
    "    \n",
    "    # Configura√ß√£o de caminhos CORRIGIDA para Parquet\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "    INPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"parquet\", \"brazilian_ecommerce_v4\", \"olist_products_dataset\")\n",
    "    OUTPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"refined\", \"products\")\n",
    "    \n",
    "    try:\n",
    "        # Leitura do Parquet CORRIGIDA\n",
    "        print(\"üì¶ Lendo dados de products (Parquet)...\")\n",
    "        products_df = spark.read.parquet(INPUT_PATH)\n",
    "        \n",
    "        # Transforma√ß√£o\n",
    "        print(\"üõ†Ô∏è Enriquecendo dados...\")\n",
    "        products_refined = transform_products(products_df)\n",
    "        \n",
    "        # Escrita com particionamento\n",
    "        print(\"üíæ Salvando dados refinados...\")\n",
    "        (products_refined.write\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"size_category\", \"photo_category\")\n",
    "            .parquet(OUTPUT_PATH))\n",
    "        \n",
    "        # Verifica√ß√£o\n",
    "        print(\"‚úÖ Processo conclu√≠do! Amostra dos dados:\")\n",
    "        products_refined.show(5)\n",
    "        \n",
    "        print(\"\\nüìä Estat√≠sticas:\")\n",
    "        products_refined.select(\n",
    "            count(\"*\").alias(\"total_products\"),\n",
    "            countDistinct(\"product_category_name\").alias(\"unique_categories\"),\n",
    "            mean(\"product_weight_g\").alias(\"avg_weight_g\"),\n",
    "            mean(\"product_volume_cm3\").alias(\"avg_volume_cm3\"),\n",
    "            mean(\"product_density_g_cm3\").alias(\"avg_density\")\n",
    "        ).show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
