{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2198fe0e",
   "metadata": {},
   "source": [
    "### Refining the data before curation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff354d9",
   "metadata": {},
   "source": [
    "#### Costumers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1a31c",
   "metadata": {},
   "source": [
    "O que √© Particionamento no Spark?\n",
    "√â uma t√©cnica de organiza√ß√£o de dados que:\n",
    "\n",
    "Divide fisicamente os arquivos por valores de colunas\n",
    "\n",
    "Cria subpastas autom√°ticas no formato nome_coluna=valor\n",
    "\n",
    "Melhora performance em consultas filtradas por essas colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6064d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark_config.py\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def get_spark_session(app_name):\n",
    "    \"\"\"Configura√ß√£o compartilhada por todos os scripts\"\"\"\n",
    "    # 1. Configura√ß√£o do Hadoop para Windows\n",
    "    hadoop_path = \"C:\\\\hadoop\"\n",
    "    os.environ['HADOOP_HOME'] = hadoop_path\n",
    "    os.environ['PATH'] = f\"{os.environ['PATH']};{hadoop_path}\\\\bin\"\n",
    "    os.environ['HADOOP_USER_NAME'] = \"user\"\n",
    "    \n",
    "    # 2. Configura√ß√µes essenciais do Spark\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "246d60e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Lendo dados de: c:\\Users\\pacie\\Desktop\\dataplatform-pipelines\\data_lake\\parquet\\brazilian_ecommerce_v4\\olist_customers_dataset\n",
      "‚ùå Erro: An error occurred while calling o62.parquet.\n",
      ": java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\n",
      "\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:218)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:132)\n",
      "\tat scala.collection.immutable.List.map(List.scala:247)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:122)\n",
      "\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:72)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:178)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:134)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:98)\n",
      "\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:563)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:420)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:286)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:249)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:280)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# customer_refinement_fixed.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "# 1. Configura√ß√£o de caminhos ABSOLUTOS\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(os.getcwd()))  # Volta um n√≠vel para o projeto\n",
    "INPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"parquet\", \"brazilian_ecommerce_v4\", \"olist_customers_dataset\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"refined\", \"customers\")  # Pasta definitiva\n",
    "\n",
    "# 2. Fun√ß√£o de transforma√ß√£o\n",
    "def transform_customers(df):\n",
    "    # Limpeza\n",
    "    df_clean = (df\n",
    "        .withColumn(\"customer_city_clean\", lower(trim(col(\"customer_city\"))))\n",
    "        .withColumn(\"customer_state_clean\", upper(trim(col(\"customer_state\"))))\n",
    "        .dropDuplicates([\"customer_id\"])\n",
    "    )\n",
    "    \n",
    "    # Cria regi√£o (exemplo simplificado)\n",
    "    return df_clean.withColumn(\n",
    "        \"customer_region\",\n",
    "        when(col(\"customer_state_clean\").isin([\"SP\", \"RJ\", \"MG\", \"ES\"]), \"Sudeste\")\n",
    "        .when(col(\"customer_state_clean\").isin([\"RS\", \"SC\", \"PR\"]), \"Sul\")\n",
    "        .when(col(\"customer_state_clean\").isin([\"MT\", \"MS\", \"GO\", \"DF\"]), \"Centro-Oeste\")\n",
    "        .when(col(\"customer_state_clean\").isin([\"AM\", \"PA\", \"AC\", \"RO\", \"RR\", \"AP\", \"TO\"]), \"Norte\")\n",
    "        .when(col(\"customer_state_clean\").isin([\"BA\", \"SE\", \"AL\", \"PE\", \"PB\", \"RN\", \"CE\", \"PI\", \"MA\"]), \"Nordeste\")\n",
    "        .otherwise(\"Outros\")  # Apenas para casos extremos n√£o mapeados\n",
    "    )\n",
    "\n",
    "# 3. Execu√ß√£o principal\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CustomerRefinement\") \\\n",
    "        .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        # Garante que a pasta de output existe\n",
    "        os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "        \n",
    "        print(\"üìÇ Lendo dados de:\", INPUT_PATH)\n",
    "        customers_df = spark.read.parquet(INPUT_PATH)\n",
    "        \n",
    "        print(\"üîÑ Transformando dados...\")\n",
    "        customers_refined = transform_customers(customers_df)\n",
    "        \n",
    "        print(\"üíæ Salvando em:\", OUTPUT_PATH)\n",
    "        (customers_refined.write\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"customer_region\", \"customer_state_clean\")  # Parti√ß√£o dupla\n",
    "            .parquet(OUTPUT_PATH))\n",
    "        \n",
    "        # Verifica√ß√£o\n",
    "        print(\"‚úÖ Estrutura criada:\")\n",
    "        for root, dirs, files in os.walk(OUTPUT_PATH):\n",
    "            print(f\"üìÅ {root.replace(BASE_DIR, '...')}\")\n",
    "            for dir in dirs:\n",
    "                if \"=\" in dir:  # Mostra apenas pastas de parti√ß√£o\n",
    "                    print(f\"   ‚îî‚îÄ‚îÄ {dir}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48202e16",
   "metadata": {},
   "source": [
    "### Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0187ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç Lendo dados de geolocaliza√ß√£o...\n",
      "üõ†Ô∏è Processando dados...\n",
      "üíæ Salvando em: c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\refined\\geolocation\n",
      "‚úÖ Dados salvos! Estrutura:\n",
      "Total de registros: 1000163\n",
      "\n",
      "Amostra dos dados transformados:\n",
      "+---------------------------+------------------+-------------------+----------------+-----------------+----------------------+-----------------------+--------+--------------------+\n",
      "|geolocation_zip_code_prefix|   geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|geolocation_city_clean|geolocation_state_clean|  region|           geo_point|\n",
      "+---------------------------+------------------+-------------------+----------------+-----------------+----------------------+-----------------------+--------+--------------------+\n",
      "|                      50760|-8.072588074383969| -34.91359211518264|          recife|               PE|                recife|                     PE|Nordeste|POINT(-34.9135921...|\n",
      "|                      50740|-8.042627210658422|-34.946004300305844|          recife|               PE|                recife|                     PE|Nordeste|POINT(-34.9460043...|\n",
      "|                      50751|-8.066622635581963|-34.917061201223355|          recife|               PE|                recife|                     PE|Nordeste|POINT(-34.9170612...|\n",
      "+---------------------------+------------------+-------------------+----------------+-----------------+----------------------+-----------------------+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Exemplo de estrutura gerada:\n",
      "üìÅ c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\refined\\geolocation\\region=Sudeste\n",
      "   Estados: ['geolocation_state_clean=ES', 'geolocation_state_clean=MG', 'geolocation_state_clean=RJ', 'geolocation_state_clean=SP']\n",
      "üìÅ c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\refined\\geolocation\\region=Nordeste\n",
      "   Estados: ['geolocation_state_clean=AL', 'geolocation_state_clean=BA', 'geolocation_state_clean=CE', 'geolocation_state_clean=MA', 'geolocation_state_clean=PB', 'geolocation_state_clean=PE', 'geolocation_state_clean=PI', 'geolocation_state_clean=RN', 'geolocation_state_clean=SE']\n",
      "üìÅ c:\\Users\\pacie\\Desktop\\Projeto A\\data_lake\\refined\\geolocation\\region=Sul\n",
      "   Estados: ['geolocation_state_clean=PR', 'geolocation_state_clean=RS', 'geolocation_state_clean=SC']\n"
     ]
    }
   ],
   "source": [
    "# geolocation_refinement.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "\n",
    "# Configura√ß√£o de caminhos\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(os.getcwd()))  # Volta um n√≠vel\n",
    "INPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"parquet\", \"brazilian_ecommerce_v4\", \"olist_geolocation_dataset\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"refined\", \"geolocation\")\n",
    "\n",
    "def transform_geolocation_data(df):\n",
    "    \"\"\"Transforma os dados de geolocaliza√ß√£o com:\n",
    "    - Limpeza de textos\n",
    "    - Categoriza√ß√£o por regi√£o\n",
    "    - Controle de qualidade\n",
    "    \"\"\"\n",
    "    # Limpeza e padroniza√ß√£o\n",
    "    df_clean = (df\n",
    "        .withColumn(\"geolocation_city_clean\", lower(trim(col(\"geolocation_city\"))))\n",
    "        .withColumn(\"geolocation_state_clean\", upper(trim(col(\"geolocation_state\"))))\n",
    "        .na.drop(subset=[\"geolocation_zip_code_prefix\"])  # Remove registros sem CEP\n",
    "    )\n",
    "    \n",
    "    # Cria regi√£o geogr√°fica (mesma l√≥gica dos clientes)\n",
    "    df_with_region = df_clean.withColumn(\n",
    "        \"region\",\n",
    "        when(col(\"geolocation_state_clean\").isin([\"SP\", \"RJ\", \"MG\", \"ES\"]), \"Sudeste\")\n",
    "        .when(col(\"geolocation_state_clean\").isin([\"RS\", \"SC\", \"PR\"]), \"Sul\")\n",
    "        .when(col(\"geolocation_state_clean\").isin([\"MT\", \"MS\", \"GO\", \"DF\"]), \"Centro-Oeste\")\n",
    "        .when(col(\"geolocation_state_clean\").isin([\"AM\", \"PA\", \"AC\", \"RO\", \"RR\", \"AP\", \"TO\"]), \"Norte\")\n",
    "        .otherwise(\"Nordeste\")\n",
    "    )\n",
    "    \n",
    "    # Cria ponto geogr√°fico (lat+lng)\n",
    "    return df_with_region.withColumn(\n",
    "        \"geo_point\",\n",
    "        format_string(\"POINT(%s %s)\", col(\"geolocation_lng\"), col(\"geolocation_lat\"))\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"GeolocationRefinement\").getOrCreate()\n",
    "    \n",
    "    try:\n",
    "        # Leitura\n",
    "        print(\"üìç Lendo dados de geolocaliza√ß√£o...\")\n",
    "        geo_df = spark.read.parquet(INPUT_PATH)\n",
    "        \n",
    "        # Transforma√ß√£o\n",
    "        print(\"üõ†Ô∏è Processando dados...\")\n",
    "        geo_refined = transform_geolocation_data(geo_df)\n",
    "        \n",
    "        # Escrita com particionamento por regi√£o/estado\n",
    "        print(\"üíæ Salvando em:\", OUTPUT_PATH)\n",
    "        (geo_refined.write\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"region\", \"geolocation_state_clean\")\n",
    "            .option(\"compression\", \"snappy\")\n",
    "            .parquet(OUTPUT_PATH))\n",
    "        \n",
    "        # Verifica√ß√£o\n",
    "        print(\"‚úÖ Dados salvos! Estrutura:\")\n",
    "        print(f\"Total de registros: {geo_refined.count()}\")\n",
    "        print(\"\\nAmostra dos dados transformados:\")\n",
    "        geo_refined.show(3)\n",
    "        \n",
    "        print(\"\\nExemplo de estrutura gerada:\")\n",
    "        for region in [\"Sudeste\", \"Nordeste\", \"Sul\"]:\n",
    "            path = os.path.join(OUTPUT_PATH, f\"region={region}\")\n",
    "            if os.path.exists(path):\n",
    "                print(f\"üìÅ {path}\")\n",
    "                print(f\"   Estados: {os.listdir(path)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Falha no processamento: {str(e)}\")\n",
    "        if 'geo_df' in locals():\n",
    "            print(\"\\nEsquema original:\", geo_df.printSchema())\n",
    "    finally:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7763f",
   "metadata": {},
   "source": [
    "### Order Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bef51d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è Verificando HADOOP_HOME em: C:\\hadoop\n",
      "‚úîÔ∏è Verificando winutils.exe em: C:\\hadoop\\bin\\winutils.exe\n",
      "üì¶ Lendo dados de order_items...\n",
      "üõ†Ô∏è Enriquecendo dados...\n",
      "üíæ Salvando dados refinados...\n",
      "‚úÖ Processo conclu√≠do! Amostra dos dados:\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+-----------+-----------+-------------+----------------+--------------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|total_value|price_range|freight_ratio|freight_category|shipping_time_period|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+-----------+-----------+-------------+----------------+--------------------+\n",
      "|8ac26cb701a7887cc...|            1|4ebb87ba41ca44632...|7a67c85e85bb2ce85...|2017-05-22 16:05:14|109.99|        18.02|     128.01|    100-150|         0.16|           M√©dio|               Tarde|\n",
      "|8ac2728285fd4228f...|            1|8b90be4893a4277a9...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|109.99|         8.27|     118.26|    100-150|         0.08|           Baixo|               Tarde|\n",
      "|8ac2728285fd4228f...|            2|fa94f25a73969e3a2...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|109.99|        16.55|     126.54|    100-150|         0.15|           M√©dio|               Tarde|\n",
      "|8ac2728285fd4228f...|            3|b01cedfa96d891427...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|259.99|        21.01|      281.0|       150+|         0.08|           Baixo|               Tarde|\n",
      "|8ac2728285fd4228f...|            4|fa94f25a73969e3a2...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|109.99|        16.55|     126.54|    100-150|         0.15|           M√©dio|               Tarde|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+-----------+-----------+-------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "üìä Estat√≠sticas:\n",
      "‚ùå Erro: name 'countDistinct' is not defined\n"
     ]
    }
   ],
   "source": [
    "# order_items_refinement.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "def get_spark_session(app_name):\n",
    "    \"\"\"\n",
    "    Configura√ß√£o compartilhada por todos os scripts, com op√ß√µes para Hadoop.\n",
    "    \n",
    "    Verifica a presen√ßa de 'winutils.exe' para evitar 'UnsatisfiedLinkError' no Windows.\n",
    "    \"\"\"\n",
    "    # 1. Configura√ß√£o do Hadoop para Windows (ajuste se necess√°rio para outros SOs)\n",
    "    # ATEN√á√ÉO: Verifique o caminho para o seu diret√≥rio Hadoop e baixe o bin√°rio\n",
    "    # do winutils.exe compat√≠vel com a sua vers√£o do Spark/Hadoop.\n",
    "    hadoop_path = \"C:\\\\hadoop\"\n",
    "    winutils_path = os.path.join(hadoop_path, \"bin\", \"winutils.exe\")\n",
    "    \n",
    "    # Adicionando debug para verificar os caminhos\n",
    "    print(f\"‚úîÔ∏è Verificando HADOOP_HOME em: {hadoop_path}\")\n",
    "    print(f\"‚úîÔ∏è Verificando winutils.exe em: {winutils_path}\")\n",
    "    \n",
    "    # Verifica se o winutils.exe existe antes de tentar configurar as vari√°veis de ambiente\n",
    "    if not os.path.exists(winutils_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"‚ùå Erro: O arquivo winutils.exe n√£o foi encontrado em '{winutils_path}'. \"\n",
    "            f\"Isso geralmente indica um problema com a vari√°vel de ambiente HADOOP_HOME \"\n",
    "            f\"ou que o bin√°rio necess√°rio n√£o est√° no local correto. \"\n",
    "            f\"Por favor, verifique se a pasta '{hadoop_path}' cont√©m a estrutura de pastas \"\n",
    "            f\"'bin' e se o 'winutils.exe' est√° dentro dela.\"\n",
    "        )\n",
    "        \n",
    "    os.environ['HADOOP_HOME'] = hadoop_path\n",
    "    os.environ['PATH'] = f\"{os.environ['PATH']};{hadoop_path}\\\\bin\"\n",
    "    os.environ['HADOOP_USER_NAME'] = \"user\"\n",
    "    \n",
    "    # 2. Configura√ß√µes essenciais do Spark\n",
    "    return SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "        .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"snappy\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "# Configura√ß√£o de caminhos\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "INPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"parquet\", \"brazilian_ecommerce_v4\", \"olist_order_items_dataset\")\n",
    "OUTPUT_PATH = os.path.join(BASE_DIR, \"data_lake\", \"refined\", \"order_items\")\n",
    "\n",
    "def transform_order_items(df):\n",
    "    \"\"\"Transforma√ß√µes principais para order_items\"\"\"\n",
    "    # 1. Limpeza b√°sica\n",
    "    df_clean = df.withColumn(\n",
    "        \"shipping_limit_date\", \n",
    "        to_timestamp(col(\"shipping_limit_date\"))\n",
    "    ).na.drop(subset=[\"order_id\", \"product_id\", \"seller_id\"])\n",
    "    \n",
    "    # 2. C√°lculo de valores totais\n",
    "    df_with_totals = df_clean.withColumn(\n",
    "        \"total_value\", \n",
    "        round(col(\"price\") + col(\"freight_value\"), 2)\n",
    "    )\n",
    "    \n",
    "    # 3. Categoriza√ß√£o por faixa de pre√ßo\n",
    "    df_with_price_range = df_with_totals.withColumn(\n",
    "        \"price_range\",\n",
    "        when(col(\"price\") < 50, \"0-50\")\n",
    "        .when((col(\"price\") >= 50) & (col(\"price\") < 100), \"50-100\")\n",
    "        .when((col(\"price\") >= 100) & (col(\"price\") < 150), \"100-150\") \n",
    "        .otherwise(\"150+\")\n",
    "    )\n",
    "    \n",
    "    # 4. Categoriza√ß√£o por frete\n",
    "    df_with_freight_category = df_with_price_range.withColumn(\n",
    "        \"freight_ratio\",\n",
    "        round(col(\"freight_value\") / col(\"price\"), 2)\n",
    "    ).withColumn(\n",
    "        \"freight_category\",\n",
    "        when(col(\"freight_ratio\") < 0.1, \"Baixo\")\n",
    "        .when((col(\"freight_ratio\") >= 0.1) & (col(\"freight_ratio\") < 0.25), \"M√©dio\")\n",
    "        .otherwise(\"Alto\")\n",
    "    )\n",
    "    \n",
    "    # 5. Adi√ß√£o de per√≠odo do dia (baseado no hor√°rio do shipping)\n",
    "    return df_with_freight_category.withColumn(\n",
    "        \"shipping_time_period\",\n",
    "        when(hour(col(\"shipping_limit_date\")).between(6, 11), \"Manh√£\")\n",
    "        .when(hour(col(\"shipping_limit_date\")).between(12, 17), \"Tarde\")\n",
    "        .when(hour(col(\"shipping_limit_date\")).between(18, 23), \"Noite\")\n",
    "        .otherwise(\"Madrugada\")\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Inicia a sess√£o Spark usando a fun√ß√£o de configura√ß√£o\n",
    "    spark = get_spark_session(\"OrderItemsRefinement\")\n",
    "    \n",
    "    try:\n",
    "        # Leitura\n",
    "        print(\"üì¶ Lendo dados de order_items...\")\n",
    "        order_items_df = spark.read.parquet(INPUT_PATH)\n",
    "        \n",
    "        # Transforma√ß√£o\n",
    "        print(\"üõ†Ô∏è Enriquecendo dados...\")\n",
    "        order_items_refined = transform_order_items(order_items_df)\n",
    "        \n",
    "        # Escrita com particionamento\n",
    "        print(\"üíæ Salvando dados refinados...\")\n",
    "        (order_items_refined.write\n",
    "            .mode(\"overwrite\")\n",
    "            .partitionBy(\"price_range\", \"freight_category\")  # Particionamento estrat√©gico\n",
    "            .parquet(OUTPUT_PATH))\n",
    "        \n",
    "        # Verifica√ß√£o\n",
    "        print(\"‚úÖ Processo conclu√≠do! Amostra dos dados:\")\n",
    "        order_items_refined.show(5)\n",
    "        \n",
    "        print(\"\\nüìä Estat√≠sticas:\")\n",
    "        order_items_refined.select(\n",
    "            mean(\"price\").alias(\"avg_price\"),\n",
    "            mean(\"freight_value\").alias(\"avg_freight\"),\n",
    "            countDistinct(\"order_id\").alias(\"unique_orders\")\n",
    "        ).show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro: {str(e)}\")\n",
    "    finally:\n",
    "        spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
